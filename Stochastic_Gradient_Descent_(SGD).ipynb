{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1Ahf4uBrAdCE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SGD:\n",
        "\tdef __init__(self, lr=0.01, max_iter=1000, batch_size=32, tol=1e-3):\n",
        "\t\t# learning rate of the SGD Optimizer\n",
        "\t\tself.learning_rate = lr\n",
        "\t\t# maximum number of iterations for SGD Optimizer\n",
        "\t\tself.max_iteration = max_iter\n",
        "\t\t# mini-batch size of the data\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\t# tolerance for convergence for the theta\n",
        "\t\tself.tolerence_convergence = tol\n",
        "\t\t# Initialize model parameters to None\n",
        "\t\tself.theta = None\n",
        "\t\t\n",
        "\tdef fit(self, X, y):\n",
        "\t\t# store dimension of input vector\n",
        "\t\tn, d = X.shape\n",
        "\t\t# Intialize random Theta for every feature\n",
        "\t\tself.theta = np.random.randn(d)\n",
        "\t\tfor i in range(self.max_iteration):\n",
        "\t\t\t# Shuffle the data\n",
        "\t\t\tindices = np.random.permutation(n)\n",
        "\t\t\tX = X[indices]\n",
        "\t\t\ty = y[indices]\n",
        "\t\t\t# Iterate over mini-batches\n",
        "\t\t\tfor i in range(0, n, self.batch_size):\n",
        "\t\t\t\tX_batch = X[i:i+self.batch_size]\n",
        "\t\t\t\ty_batch = y[i:i+self.batch_size]\n",
        "\t\t\t\tgrad = self.gradient(X_batch, y_batch)\n",
        "\t\t\t\tself.theta -= self.learning_rate * grad\n",
        "\t\t\t# Check for convergence\n",
        "\t\t\tif np.linalg.norm(grad) < self.tolerence_convergence:\n",
        "\t\t\t\tbreak\n",
        "\t# define a gradient functon for calculating gradient\n",
        "\t# of the data\n",
        "\tdef gradient(self, X, y):\n",
        "\t\tn = len(y)\n",
        "\t\t# predict target value by taking taking\n",
        "\t\t# taking dot product of dependent and theta value\n",
        "\t\ty_pred = np.dot(X, self.theta)\n",
        "\t\t\n",
        "\t\t# calculate error between predict and actual value\n",
        "\t\terror = y_pred - y\n",
        "\t\tgrad = np.dot(X.T, error) / n\n",
        "\t\treturn grad\n",
        "\t\n",
        "\tdef predict(self, X):\n",
        "\t\t# prdict y value using calculated theta value\n",
        "\t\ty_pred = np.dot(X, self.theta)\n",
        "\t\treturn y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create random dataset with 100 rows and 5 columns\n",
        "X = np.random.randn(100, 5)\n",
        "# create corresponding target value by adding random\n",
        "# noise in the dataset\n",
        "y = np.dot(X, np.array([1, 2, 3, 4, 5]))\\\n",
        "\t+ np.random.randn(100) * 0.1\n",
        "# Create an instance of the SGD class\n",
        "model = SGD(lr=0.01, max_iter=1000,\n",
        "\t\t\tbatch_size=32, tol=1e-3)\n",
        "model.fit(X, y)\n",
        "# Predict using predict method from model\n",
        "y_pred = model.predict(X)"
      ],
      "metadata": {
        "id": "Xe1RkoGzA1W3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating error in predictions\n",
        "mae_error = np.sum(np.abs(y - y_pred) / y.shape[0])\n",
        "print(\"Mean absolute error:\", mae_error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq37E42iC0VJ",
        "outputId": "32d479d0-7608-4183-8712-cb77b2b355e9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean absolute error: 0.08042292928784861\n"
          ]
        }
      ]
    }
  ]
}