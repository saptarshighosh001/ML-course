{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Random search is a method in which random combinations of hyperparameters are selected and used to train a model. The best random hyperparameter combinations are used. There are a few similarities between a random search and a grid search."
      ],
      "metadata": {
        "id": "YR5_eFMazMDH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Uc_1l4lWWXqm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "QCZT7Fq2WzTN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestRegressor(random_state = 35)"
      ],
      "metadata": {
        "id": "VGntW_rNXCpV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "metadata": {
        "id": "3WrdzxHKXMUl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_estimators = [int(x) for x in np.linspace(start = 1, stop = 20, num = 20)] # number of trees in the random forest\n",
        "max_features = ['auto', 'sqrt'] # number of features in consideration at every split\n",
        "max_depth = [int(x) for x in np.linspace(10, 120, num = 12)] # maximum number of levels allowed in each decision tree\n",
        "min_samples_split = [2, 6, 10] # minimum sample number to split a node\n",
        "min_samples_leaf = [1, 3, 4] # minimum sample number that can be stored in a leaf node\n",
        "bootstrap = [True, False] # method used to sample data points\n",
        "\n",
        "random_grid = {'n_estimators': n_estimators, 'max_features': max_features, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}"
      ],
      "metadata": {
        "id": "TLjKPG2Cggl7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly to our grid search implementation, we will carry out cross-validation in a random search. This is enabled by RandomizedSearchCV. By specifying cv=5, we train a model 5 times using cross-validation.\n",
        "Furthermore, when we carried out grid search, we had verbose=0 to avoid slowing down our algorithm. In this case, we can use verbose=2 to have a glimpse of the logging information generated.\n",
        "\n",
        "We have the n_iter parameter that allows us to carry out $n$ different iterations, when n_jobs = -1, all CPUs are put to use."
      ],
      "metadata": {
        "id": "AU_qiNmnrbpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=35, n_jobs = -1)"
      ],
      "metadata": {
        "id": "Bc0NpJkghQzW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_random.fit(X,y)\n",
        "\n",
        "# this prints the contents of the parameters in the random grid\n",
        "print ('Random grid: ', random_grid, '\\n')\n",
        "\n",
        "# print the best parameters\n",
        "print ('Best Parameters: ', rf_random.best_params_, ' \\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt-ZfGlghfeF",
        "outputId": "1d1186ef-9d08-4fc1-e5a6-c729b2e845e8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
            "Random grid:  {'n_estimators': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120], 'min_samples_split': [2, 6, 10], 'min_samples_leaf': [1, 3, 4], 'bootstrap': [True, False]} \n",
            "\n",
            "Best Parameters:  {'n_estimators': 10, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 70, 'bootstrap': True}  \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output gives the best parameters as 10 for n_estimators and min_samples_split. It also gives 4 for min_samples_leaf, auto for max_features, 70 for max_depth, and true for bootstrap."
      ],
      "metadata": {
        "id": "uF2yYGi8qMLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Search vs. Grid Search**\n",
        "(i) Higher dimensionality leads to a greater number of iterations. With grid search, the greater the dimensionality, the greater the number of hyperparameter combinations to search for. Hence, it is better to go with the random search, which will lead to a reduction in the number of parameters.\n",
        "\n",
        "\n",
        "(ii) The random search model can be trained on the optimized parameters in a much shorter time than when using grid search. This also results in much more efficient computational power being used in comparison to grid search."
      ],
      "metadata": {
        "id": "B18mCdoJur-8"
      }
    }
  ]
}