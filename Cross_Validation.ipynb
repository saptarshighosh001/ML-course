{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Cross Validation**\n",
        "\n",
        "When adjusting models we are aiming to increase overall model performance on unseen data. Hyperparameter tuning can lead to much better performance on test sets. However, optimizing parameters to the test set can lead information leakage causing the model to preform worse on unseen data. To correct for this we can perform cross validation.\n",
        "\n",
        "To better understand CV, we will be performing different methods on the iris dataset. Let us first load in and separate the data."
      ],
      "metadata": {
        "id": "mChMIKmtTqau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-Fold**\n",
        "\n",
        "The training data used in the model is split, into k number of smaller sets, to be used to validate the model. The model is then trained on k-1 folds of training set. The remaining fold is then used as a validation set to evaluate the model."
      ],
      "metadata": {
        "id": "mPs94-FcTCtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "k_folds = KFold(n_splits = 5)\n",
        "\n",
        "scores = cross_val_score(clf, X, y, cv = k_folds)\n",
        "\n",
        "print(\"Cross Validation Scores: \", scores)\n",
        "print(\"Average CV Score: \", scores.mean())\n",
        "print(\"Number of CV Scores used in Average: \", len(scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOiSWMxhSbdN",
        "outputId": "f016e417-0cfc-4cc4-d119-fe64dc0ad8ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Validation Scores:  [1.         1.         0.83333333 0.93333333 0.8       ]\n",
            "Average CV Score:  0.9133333333333333\n",
            "Number of CV Scores used in Average:  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stratified K-Fold**\n",
        "\n",
        "In cases where classes are imbalanced we need a way to account for the imbalance in both the train and validation sets. To do so we can stratify the target classes, meaning that both sets will have an equal proportion of all classes."
      ],
      "metadata": {
        "id": "YAPNCx80vHze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "sk_folds = StratifiedKFold(n_splits = 5)\n",
        "\n",
        "scores = cross_val_score(clf, X, y, cv = sk_folds)\n",
        "\n",
        "print(\"Cross Validation Scores: \", scores)\n",
        "print(\"Average CV Score: \", scores.mean())\n",
        "print(\"Number of CV Scores used in Average: \", len(scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yjysMk8vO_4",
        "outputId": "dd0ff8dc-7803-43e2-8bc7-660caa9b8ab9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Validation Scores:  [0.96666667 0.96666667 0.9        0.93333333 1.        ]\n",
            "Average CV Score:  0.9533333333333334\n",
            "Number of CV Scores used in Average:  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the number of folds is the same, the average cross-validation (CV) increases from the basic k-fold when making sure there are stratified classes."
      ],
      "metadata": {
        "id": "RsLjeMPkv952"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Leave-One-Out (LOO)**\n",
        "\n",
        "Instead of selecting the number of splits in the training data set like k-fold LeaveOneOut, utilize 1 observation to validate and n-1 observations to train. This method is an exaustive technique."
      ],
      "metadata": {
        "id": "bOW2t3L3Smmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
        "\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "scores = cross_val_score(clf, X, y, cv = loo)\n",
        "\n",
        "print(\"Cross Validation Scores: \", scores)\n",
        "print(\"Average CV Score: \", scores.mean())\n",
        "print(\"Number of CV Scores used in Average: \", len(scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKmC7ubKRAjt",
        "outputId": "299c9da1-7d17-4d9d-87e8-744df30cf026"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Validation Scores:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1.]\n",
            "Average CV Score:  0.94\n",
            "Number of CV Scores used in Average:  150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Leave-P-Out (LPO)**\n",
        "\n",
        "Leave-P-Out is simply a nuanced diffence to the Leave-One-Out idea, in that we can select the number of p to use in our validation set."
      ],
      "metadata": {
        "id": "zJJieTRsHHuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import LeavePOut, cross_val_score\n",
        "\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "lpo = LeavePOut(p=2)\n",
        "\n",
        "scores = cross_val_score(clf, X, y, cv = lpo)\n",
        "\n",
        "print(\"Cross Validation Scores: \", scores)\n",
        "print(\"Average CV Score: \", scores.mean())\n",
        "print(\"Number of CV Scores used in Average: \", len(scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXSHaNZPGXTt",
        "outputId": "b08913da-6633-4e28-f5dd-1c31e8d2be54"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Validation Scores:  [1. 1. 1. ... 1. 1. 1.]\n",
            "Average CV Score:  0.9382997762863534\n",
            "Number of CV Scores used in Average:  11175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shuffle Split**\n",
        "\n",
        "Unlike KFold, ShuffleSplit leaves out a percentage of the data, not to be used in the train or validation sets. To do so we must decide what the train and test sizes are, as well as the number of splits."
      ],
      "metadata": {
        "id": "DBphjo3VFNAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
        "\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "ss = ShuffleSplit(train_size=0.6, test_size=0.3, n_splits = 5)\n",
        "\n",
        "scores = cross_val_score(clf, X, y, cv = ss)\n",
        "\n",
        "print(\"Cross Validation Scores: \", scores)\n",
        "print(\"Average CV Score: \", scores.mean())\n",
        "print(\"Number of CV Scores used in Average: \", len(scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDDe5O8DFIPT",
        "outputId": "af9789d9-b0aa-4da2-95e5-8fe133d03517"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Validation Scores:  [0.97777778 0.97777778 1.         0.91111111 0.93333333]\n",
            "Average CV Score:  0.96\n",
            "Number of CV Scores used in Average:  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among different cross-validation techniques, we can observe that the Shuffle Split technique provides the best average CV score."
      ],
      "metadata": {
        "id": "Qssx5wNFVSNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the few important cross-validation (CV) methods that can be applied to models. There are many more cross-validation classes, with most models having their own class. You can get the details from sklearn cross-validation."
      ],
      "metadata": {
        "id": "nVpeWsM_V0jM"
      }
    }
  ]
}